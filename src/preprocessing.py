"""
–ú–æ–¥—É–ª—å –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö

–≠—Ç–∞–ø—ã –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏:
1. –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ ‚Äî —É–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤, –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
2. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è ‚Äî —Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —Å–ª–æ–≤–∞/—Ç–æ–∫–µ–Ω—ã
3. –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è ‚Äî –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –Ω–∞—á–∞–ª—å–Ω–æ–π —Ñ–æ—Ä–º–µ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
4. –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ ‚Äî –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –≤ —á–∏—Å–ª–æ–≤—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
5. –ü–∞–¥–¥–∏–Ω–≥ ‚Äî –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –¥–æ –æ–¥–Ω–æ–π –¥–ª–∏–Ω—ã
"""

import re
import numpy as np
import pandas as pd
from typing import List, Tuple, Dict, Optional
from pathlib import Path

import sys
sys.path.insert(0, str(Path(__file__).parent.parent))
from config import PREPROCESSING_CONFIG, CATEGORIES


class TextPreprocessor:
    """–ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ—Å–µ—Ç–∏"""
    
    # –°—Ç–æ–ø-—Å–ª–æ–≤–∞ —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ (—á–∞—Å—Ç–∏—á–Ω—ã–π —Å–ø–∏—Å–æ–∫)
    STOP_WORDS = {
        "–∏", "–≤", "–≤–æ", "–Ω–µ", "—á—Ç–æ", "–æ–Ω", "–Ω–∞", "—è", "—Å", "—Å–æ", "–∫–∞–∫",
        "–∞", "—Ç–æ", "–≤—Å–µ", "–æ–Ω–∞", "—Ç–∞–∫", "–µ–≥–æ", "–Ω–æ", "–¥–∞", "—Ç—ã", "–∫", 
        "—É", "–∂–µ", "–≤—ã", "–∑–∞", "–±—ã", "–ø–æ", "—Ç–æ–ª—å–∫–æ", "–µ—ë", "–º–Ω–µ", "–±—ã–ª–æ",
        "–≤–æ—Ç", "–æ—Ç", "–º–µ–Ω—è", "–µ—â—ë", "–Ω–µ—Ç", "–æ", "–∏–∑", "–µ–º—É", "—Ç–µ–ø–µ—Ä—å",
        "–∫–æ–≥–¥–∞", "—É–∂–µ", "–¥–ª—è", "–≤–∞–º", "–≤–µ–¥—å", "—Ç–∞–º", "–º–æ–π", "–µ—Å–ª–∏", "—ç—Ç–æ—Ç"
    }
    
    def __init__(self, config: Dict = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
        
        Args:
            config: –°–ª–æ–≤–∞—Ä—å —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏
        """
        self.config = config or PREPROCESSING_CONFIG
        self.max_words = self.config["max_words"]
        self.max_sequence_length = self.config["max_sequence_length"]
        
        # –°–ª–æ–≤–∞—Ä–∏ –¥–ª—è –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è
        self.word_to_index: Dict[str, int] = {"<PAD>": 0, "<UNK>": 1}
        self.index_to_word: Dict[int, str] = {0: "<PAD>", 1: "<UNK>"}
        self.word_counts: Dict[str, int] = {}
        
        # –ö–æ–¥–∏—Ä–æ–≤—â–∏–∫–∏ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        self.category_to_index: Dict[str, int] = {}
        self.subject_to_index: Dict[str, int] = {}
        
        self.is_fitted = False
        
    def clean_text(self, text: str) -> str:
        """
        –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç —à—É–º–∞
        
        Args:
            text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
            
        Returns:
            –û—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        """
        # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        text = text.lower()
        
        # –£–¥–∞–ª–µ–Ω–∏–µ HTML-—Ç–µ–≥–æ–≤
        text = re.sub(r'<[^>]+>', '', text)
        
        # –£–¥–∞–ª–µ–Ω–∏–µ URL
        text = re.sub(r'http\S+|www\S+', '', text)
        
        # –£–¥–∞–ª–µ–Ω–∏–µ email
        text = re.sub(r'\S+@\S+', '', text)
        
        # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –±—É–∫–≤—ã –∏ –ø—Ä–æ–±–µ–ª—ã
        text = re.sub(r'[^–∞-—è—ëa-z\s]', ' ', text)
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø—Ä–æ–±–µ–ª–æ–≤
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def tokenize(self, text: str, remove_stopwords: bool = False) -> List[str]:
        """
        –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
        
        Args:
            text: –û—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
            remove_stopwords: –£–¥–∞–ª—è—Ç—å –ª–∏ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤
        """
        tokens = text.split()
        
        if remove_stopwords:
            tokens = [t for t in tokens if t not in self.STOP_WORDS]
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤
        tokens = [t for t in tokens if len(t) > 1]
        
        return tokens
    
    def fit(self, texts: List[str], categories: List[str] = None, 
            subjects: List[str] = None):
        """
        –û–±—É—á–µ–Ω–∏–µ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ –Ω–∞ —Ç–µ–∫—Å—Ç–∞—Ö
        
        –°—Ç—Ä–æ–∏—Ç —Å–ª–æ–≤–∞—Ä—å –Ω–∞ –æ—Å–Ω–æ–≤–µ —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç–∏ —Å–ª–æ–≤
        
        Args:
            texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            categories: –°–ø–∏—Å–æ–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–π
            subjects: –°–ø–∏—Å–æ–∫ —Å—É–±—ä–µ–∫—Ç–æ–≤
        """
        # –ü–æ–¥—Å—á—ë—Ç —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç–∏ —Å–ª–æ–≤
        for text in texts:
            cleaned = self.clean_text(text)
            tokens = self.tokenize(cleaned)
            for token in tokens:
                self.word_counts[token] = self.word_counts.get(token, 0) + 1
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç–∏ –∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è
        sorted_words = sorted(
            self.word_counts.items(), 
            key=lambda x: x[1], 
            reverse=True
        )[:self.max_words - 2]  # -2 –¥–ª—è PAD –∏ UNK
        
        for word, _ in sorted_words:
            idx = len(self.word_to_index)
            self.word_to_index[word] = idx
            self.index_to_word[idx] = word
        
        # –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        if categories:
            unique_categories = sorted(set(categories))
            self.category_to_index = {cat: i for i, cat in enumerate(unique_categories)}
        
        # –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—É–±—ä–µ–∫—Ç–æ–≤
        if subjects:
            unique_subjects = sorted(set(subjects))
            self.subject_to_index = {subj: i for i, subj in enumerate(unique_subjects)}
        
        self.is_fitted = True
        print(f"‚úÖ –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –æ–±—É—á–µ–Ω:")
        print(f"   –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {len(self.word_to_index)}")
        print(f"   –ö–∞—Ç–µ–≥–æ—Ä–∏–π: {len(self.category_to_index)}")
        print(f"   –°—É–±—ä–µ–∫—Ç–æ–≤: {len(self.subject_to_index)}")
    
    def text_to_sequence(self, text: str) -> List[int]:
        """
        –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏–Ω–¥–µ–∫—Å–æ–≤
        
        Args:
            text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
            
        Returns:
            –°–ø–∏—Å–æ–∫ –∏–Ω–¥–µ–∫—Å–æ–≤ —Å–ª–æ–≤
        """
        if not self.is_fitted:
            raise ValueError("–ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –Ω–µ –æ–±—É—á–µ–Ω. –í—ã–∑–æ–≤–∏—Ç–µ fit() —Å–Ω–∞—á–∞–ª–∞.")
        
        cleaned = self.clean_text(text)
        tokens = self.tokenize(cleaned)
        
        sequence = []
        for token in tokens:
            idx = self.word_to_index.get(token, 1)  # 1 = UNK
            sequence.append(idx)
        
        return sequence
    
    def pad_sequence(self, sequence: List[int]) -> np.ndarray:
        """
        –ü–∞–¥–¥–∏–Ω–≥ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–æ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –¥–ª–∏–Ω—ã
        
        Args:
            sequence: –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏–Ω–¥–µ–∫—Å–æ–≤
            
        Returns:
            –í—ã—Ä–æ–≤–Ω–µ–Ω–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å
        """
        if len(sequence) > self.max_sequence_length:
            # –û–±—Ä–µ–∑–∞–µ–º —Å –∫–æ–Ω—Ü–∞ (—Å–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–∞—á–∞–ª–æ)
            return np.array(sequence[:self.max_sequence_length])
        else:
            # –î–æ–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏ (pre-padding)
            padding = [0] * (self.max_sequence_length - len(sequence))
            return np.array(padding + sequence)
    
    def encode_category(self, category: str) -> int:
        """–ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –≤ –∏–Ω–¥–µ–∫—Å"""
        return self.category_to_index.get(category, 0)
    
    def encode_subject(self, subject: str) -> int:
        """–ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—É–±—ä–µ–∫—Ç–∞ –≤ –∏–Ω–¥–µ–∫—Å"""
        return self.subject_to_index.get(subject, 0)
    
    def transform(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
        """
        –ü–æ–ª–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞
        
        Args:
            df: DataFrame —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ review_text, rating, category, subject_id,
                sentiment_class, reputation_index
                
        Returns:
            –ö–æ—Ä—Ç–µ–∂ –∏–∑:
            - X_text: –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ç–æ–∫–µ–Ω–æ–≤ [n_samples, max_seq_len]
            - X_rating: –æ—Ü–µ–Ω–∫–∏ [n_samples, 1]
            - X_category: –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ [n_samples, 1]
            - y_class: –∫–ª–∞—Å—Å—ã —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ [n_samples]
            - y_index: –∏–Ω–¥–µ–∫—Å—ã —Ä–µ–ø—É—Ç–∞—Ü–∏–∏ [n_samples]
        """
        if not self.is_fitted:
            raise ValueError("–ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –Ω–µ –æ–±—É—á–µ–Ω. –í—ã–∑–æ–≤–∏—Ç–µ fit() —Å–Ω–∞—á–∞–ª–∞.")
        
        n_samples = len(df)
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤
        X_text = np.zeros((n_samples, self.max_sequence_length), dtype=np.int32)
        for i, text in enumerate(df["review_text"]):
            sequence = self.text_to_sequence(text)
            X_text[i] = self.pad_sequence(sequence)
        
        # –û—Ü–µ–Ω–∫–∏ (–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫ 0-1)
        X_rating = (df["rating"].values - 1) / 4  # [1,5] -> [0,1]
        X_rating = X_rating.reshape(-1, 1).astype(np.float32)
        
        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ (one-hot encoding)
        n_categories = len(self.category_to_index)
        X_category = np.zeros((n_samples, n_categories), dtype=np.float32)
        for i, cat in enumerate(df["category"]):
            cat_idx = self.encode_category(cat)
            X_category[i, cat_idx] = 1.0
        
        # –¶–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
        y_class = df["sentiment_class"].values.astype(np.int32)
        y_index = df["reputation_index"].values.astype(np.float32)
        
        return X_text, X_rating, X_category, y_class, y_index
    
    def get_vocab_size(self) -> int:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è"""
        return len(self.word_to_index)
    
    def get_num_categories(self) -> int:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–π"""
        return len(self.category_to_index)
    
    def decode_sequence(self, sequence: np.ndarray) -> str:
        """
        –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –æ–±—Ä–∞—Ç–Ω–æ –≤ —Ç–µ–∫—Å—Ç
        
        Args:
            sequence: –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏–Ω–¥–µ–∫—Å–æ–≤
            
        Returns:
            –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        """
        words = []
        for idx in sequence:
            if idx == 0:  # PAD
                continue
            word = self.index_to_word.get(idx, "<UNK>")
            words.append(word)
        return " ".join(words)


def main():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞"""
    from data_generator import ReviewDataGenerator
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    generator = ReviewDataGenerator({"num_samples": 100, "num_subjects": 10, "random_seed": 42})
    df = generator.generate_dataset()
    
    # –°–æ–∑–¥–∞—ë–º –∏ –æ–±—É—á–∞–µ–º –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
    preprocessor = TextPreprocessor()
    preprocessor.fit(
        texts=df["review_text"].tolist(),
        categories=df["category"].tolist(),
        subjects=df["subject_id"].tolist()
    )
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ
    X_text, X_rating, X_category, y_class, y_index = preprocessor.transform(df)
    
    print("\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏:")
    print(f"   X_text shape: {X_text.shape}")
    print(f"   X_rating shape: {X_rating.shape}")
    print(f"   X_category shape: {X_category.shape}")
    print(f"   y_class shape: {y_class.shape}")
    print(f"   y_index shape: {y_index.shape}")
    
    # –ü—Ä–∏–º–µ—Ä –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è
    print("\nüîÑ –ü—Ä–∏–º–µ—Ä –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è:")
    sample_text = df["review_text"].iloc[0]
    print(f"   –û—Ä–∏–≥–∏–Ω–∞–ª: {sample_text}")
    sequence = preprocessor.text_to_sequence(sample_text)
    print(f"   –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {sequence[:10]}...")
    decoded = preprocessor.decode_sequence(X_text[0])
    print(f"   –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–æ: {decoded}")


if __name__ == "__main__":
    main()
